{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural style transfer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPoAqlTh+mpvqJSbbiKNL3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sambhav300899/Neural-Style-Transfer-TF/blob/master/demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KxbhcVGUXBD",
        "colab_type": "text"
      },
      "source": [
        "**Neural Style Transfer** is a Deep Learning technique which is used to transfer the style of one image to another, at the same time retaining the content of the original image. Here the technique proposed by Leon A. Gatys, Alexander S. Ecker, Matthias Bethge in https://arxiv.org/abs/1508.06576 is used.            \n",
        "\n",
        "NOTE - In the paper the L-BFGS was used for optimisation but Adam has been used here instead.\n",
        "\n",
        "The basic steps are -       \n",
        "1. Take a content and style image.\n",
        "2. Take a target image which will be the output.\n",
        "3. Calculate the content and style loss for the target image to see how much the style and content matches the original images.\n",
        "4. Calculate gradients for the target image.\n",
        "5. Update the target image.\n",
        "6. Repeat steps 3 to 5 for n iterations.\n",
        "\n",
        "To perform step 3 we need some way to extract the content and style of the original images, this is done by using a pretrained CNN, as a CNN is just a collection of filters which extract relevant features from an image. The layers near the input of a CNN extract features like edges and the higher layers near the output extract features like ears and noses. One more important thing to note is that a that CNN does not learn to encode what an image is but it actually learns to encode what image represents, which means that it extracts the style and content of the image. \n",
        "\n",
        "Here VGG19 and VGG16 have been used for feature extraction.\n",
        "\n",
        "<img src=\"https://datascience-enthusiast.com/figures/louvre_generated.png\" alt=\"drawing\"/>\n",
        "\n",
        "\n",
        "\n",
        "**VGG19 Architecture**         \n",
        "![alt text](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg)\n",
        "\n",
        "**VGG16 Architecture**                       \n",
        "![alt text](https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6dYvwWKCRfW",
        "colab_type": "text"
      },
      "source": [
        "Clone the repository for style and content image samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isrF5C7sCIX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Sambhav300899/Neural-Style-Transform-TF.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5tckw933lq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r Neural-Style-Transform-TF/content .\n",
        "!cp -r Neural-Style-Transform-TF/styles ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZAzegY9CyiT",
        "colab_type": "text"
      },
      "source": [
        "import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SsPkKcvB8tT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import cv2\n",
        "import argparse\n",
        "import progressbar\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.applications import vgg19, vgg16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6AKB-gzaYB7",
        "colab_type": "text"
      },
      "source": [
        "**Visualising the activations**      \n",
        "Let's visualise the activations of VGG19, we will use a picture of a dog due to the abundance of dog pictures in imagenet. We visualise the activations for the style and content layers, and can see that the upper layers of the network extract gradients which are similar to brush strokes in a painting and the final layer contains more information about the content. One interesting thing to note here is that as we move further in the network the activations get sparser, this is due to the fact that the network is filtering out information which is not required and only firing for the class of the object.\n",
        "\n",
        "The layers that we will use for content loss are - 'block5_conv2'    \n",
        "The layers that we will use for style loss are - 'block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7iYgXtDa9yj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to get the model and its outputs\n",
        "def get_model(shape, layers):\n",
        "    base = vgg19.VGG19(include_top = False, weights = 'imagenet', input_shape = shape)\n",
        "    base.summary()\n",
        "\n",
        "    #get the outputs from the model\n",
        "    outputs = [base.get_layer(name).output for name in layers]\n",
        "\n",
        "    model = Model(base.input, outputs)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    #the style and content layers that we use for style transfer\n",
        "    layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1', 'block5_conv2']\n",
        "\n",
        "    #lets load the image and resize it to make representation easier\n",
        "    img = cv2.imread('content/dog.jpg')\n",
        "    img = cv2.resize(img, (512, 512))\n",
        "    model = get_model(img.shape, layers)\n",
        "\n",
        "    input = np.expand_dims(img.astype('float'), axis = 0)\n",
        "    input = vgg19.preprocess_input(input)\n",
        "\n",
        "    feature_maps = model(input)\n",
        "    imgs_per_row = 16\n",
        "\n",
        "    for layer, map in zip(layers, feature_maps):\n",
        "        features = map.shape[-1]\n",
        "        size = map.shape[1]\n",
        "        n_cols = features // imgs_per_row\n",
        "\n",
        "        display_grid = np.zeros((size * n_cols, imgs_per_row * size))\n",
        "\n",
        "        for col in range(n_cols):\n",
        "            for row in range(imgs_per_row):\n",
        "                channel_image = map.numpy()[0, :, :, col * imgs_per_row + row]\n",
        "                channel_image -= channel_image.mean()\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "                channel_image = np.clip(channel_image, 0, 255).astype(np.uint8)\n",
        "\n",
        "                display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "        scale = 1. / size\n",
        "        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
        "        plt.title(layer)\n",
        "        plt.grid(False)\n",
        "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqhZHLxtDaMS",
        "colab_type": "text"
      },
      "source": [
        "**Define utility functions** \n",
        "\n",
        "Here the preprocessing is used to make the input suitable for passing to the network.\n",
        "\n",
        "VGG19 and VGG16 apply extra preprocessing to the image when it is passed to the network, to remove this and obtain the image in a suitable format we use the deprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vD-Sdw6DDp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pre_proc(img):\n",
        "    '''\n",
        "    pre process the image to input to the network\n",
        "\n",
        "    args - \n",
        "    img : input image to be pre processed\n",
        "    '''\n",
        "    img = np.array(img).astype('float')\n",
        "    img = np.expand_dims(img, axis = 0)\n",
        "\n",
        "    return img\n",
        "\n",
        "def deprocess_img_vgg(processed_img):\n",
        "  '''\n",
        "  de process the image after\n",
        "  passing through a VGG model\n",
        "\n",
        "  args -\n",
        "  preprocessed_image - image to be de processed\n",
        "  '''\n",
        "  x = processed_img.copy()\n",
        "\n",
        "  if len(x.shape) == 4:\n",
        "    x = np.squeeze(x, 0)\n",
        "\n",
        "  x[:, :, 0] += 103.939\n",
        "  x[:, :, 1] += 116.779\n",
        "  x[:, :, 2] += 123.68\n",
        "  x = x[:, :, ::-1]\n",
        "\n",
        "  x = np.clip(x, 0, 255).astype('uint8')\n",
        "  return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QGdSJbfDkIb",
        "colab_type": "text"
      },
      "source": [
        "**Define the losses**\n",
        "\n",
        "**content loss**         \n",
        "The content loss is just the Mean Squared Error between the feature map of the content image and the target image. We use this loss to keep the content the same in both of the images.  \n",
        "The content loss is given by - \n",
        "![alt text](https://miro.medium.com/max/1050/1*34xPuexhGCHT7xZ17wVvDQ.jpeg)\n",
        "\n",
        "**style loss**           \n",
        "To find the style loss we use multiple output feature maps as specified above. For calculating the style loss we can't directly use the MSE between the feature maps, because style is more of a similarity measure and two images of the same style need not be exactly the same. We calculate the gram matrix of the feature maps and then find the difference between them to find the style loss. The basic idea of a gram matrix is that it finds the correlation between the channels of an image by finding the dot product between all of the channels. As we know dot product is a measure of correlation between two vectors. If we find the difference between the gram matrices of feature maps of target image and style image then we end up minimising the loss. \n",
        "\n",
        "The loss between gram matrices of one single layer is given by - \n",
        "![alt text](https://miro.medium.com/max/1188/1*IoozR3xGzaSqtEqGEKcWMQ.jpeg)\n",
        "\n",
        "The total style loss is given by(wl is the loss weight of each layer)              \n",
        "![alt text](https://miro.medium.com/max/533/1*n7wIYY399mOdO9jJGM6aoA.jpeg)\n",
        "\n",
        "**Variation Loss**               \n",
        "One downside to this implementation is that it produces a lot of high frequency artifacts. We can reduce this using variational loss which encourages spatial continuity and thus avoids over pixelated results. This loss is already implemented in the main tensorflow and can be called directly when calculating the loss\n",
        "\n",
        "**Finally our total loss is a weighted average of these three losses.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zwfCQ2kDgu5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  '''\n",
        "  calculates the gram matrix of a given input tensor\n",
        "  \n",
        "  args - \n",
        "  input_tensor - input tensor\n",
        "  '''\n",
        "  channels = int(input_tensor.shape[-1])\n",
        "  a = tf.reshape(input_tensor, [-1, channels])\n",
        "  n = tf.shape(a)[0]\n",
        "  gram = tf.matmul(a, a, transpose_a=True)\n",
        "  return gram / tf.cast(n, tf.float32)\n",
        "\n",
        "def calc_style_loss(base_style, gram_target):\n",
        "  '''\n",
        "  calculates the style loss for a single layer\n",
        "\n",
        "  args - \n",
        "  base_style - the feature maps of the target image\n",
        "  gram_target - the gram matrix of the style image\n",
        "  '''\n",
        "  gram_style = gram_matrix(base_style)\n",
        "\n",
        "  return tf.reduce_mean(tf.square(gram_style - gram_target))\n",
        "\n",
        "def calc_content_loss(img_op, img_content):\n",
        "  '''\n",
        "  calculate the content loss for a single layer\n",
        "\n",
        "  args - \n",
        "  img_op : feature maps of target image\n",
        "  img_content : feature maps of content image\n",
        "  '''\n",
        "  return tf.reduce_mean(tf.square(img_op - img_content))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8DYZvjezv_A",
        "colab_type": "text"
      },
      "source": [
        "**Define the style transfer class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbk1XWDpDubx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class style_transfer():\n",
        "    '''\n",
        "    A class containing the functions related to style transfer\n",
        "\n",
        "    init args -\n",
        "    model_name : can be different model types, eg vgg19, vgg16 etc.\n",
        "    content_path : path to input content image\n",
        "    style_path : path to input style image\n",
        "    '''\n",
        "    def __init__(self, model_name, content_path, style_path):\n",
        "        #read images\n",
        "        self.style = cv2.imread(style_path)\n",
        "        self.content = cv2.imread(content_path)\n",
        "\n",
        "        print ('STYLE IMAGE')\n",
        "        cv2_imshow(self.style)\n",
        "        print ('\\n\\nCONTENT IMAGE')\n",
        "        cv2_imshow(self.content)\n",
        "\n",
        "        self.style = cv2.resize(self.style, (self.content.shape[1], self.content.shape[0]))\n",
        "\n",
        "        self.h, self.w = self.style.shape[:2]\n",
        "\n",
        "        #get the model\n",
        "        self.make_model(model_name)\n",
        "\n",
        "    def get_feature_maps(self):\n",
        "        '''\n",
        "        get the intermediate feature outputs required to\n",
        "        calculate the style and content loss\n",
        "        '''\n",
        "        #run the model on style and content images\n",
        "        output = self.model(self.create_ip())\n",
        "\n",
        "        #collect style and content features\n",
        "        style_features = [style_layer[0] for style_layer in output[len(self.content_layers):]]\n",
        "        content_features = [content_layer[1] for content_layer in output[:len(self.content_layers)]]\n",
        "\n",
        "        return style_features, content_features\n",
        "\n",
        "    def set_loss_weights(self, content_weight, style_weight, variation_weight):\n",
        "        '''\n",
        "        set the weights for how much each loss contributes to the total loss\n",
        "\n",
        "        args -\n",
        "        content_weight : weight param for content loss\n",
        "        style_weight : weight param for style loss\n",
        "        variation_weight : weight param for variation loss\n",
        "        '''\n",
        "        self.content_weight = content_weight\n",
        "        self.style_weight = style_weight\n",
        "        self.variation_weight = variation_weight\n",
        "\n",
        "    def make_model(self, model_name):\n",
        "        '''\n",
        "        create the model according to the input model specified and\n",
        "        select the layers to be used for style and content loss\n",
        "        also set the loss weights according to the model\n",
        "\n",
        "        args -\n",
        "        model_name : type of model. e.g - vgg16, vgg19\n",
        "        '''\n",
        "        if model_name.lower() == 'vgg19':\n",
        "            #get the pretrained model\n",
        "            base = vgg19.VGG19(input_shape = (self.h, self.w, 3), include_top = False, weights = 'imagenet')\n",
        "\n",
        "            #setting the content and style images\n",
        "            self.content_layers = ['block5_conv2']\n",
        "            self.style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "\n",
        "            #get the outputs from the model\n",
        "            content_outputs = [base.get_layer(name).output for name in self.content_layers]\n",
        "            style_outputs = [base.get_layer(name).output for name in self.style_layers]\n",
        "            model_outputs = content_outputs + style_outputs\n",
        "\n",
        "            #set loss weights, deprocessing function and processing function\n",
        "            self.set_loss_weights(1e3, 1e-2, 30)\n",
        "            self.de_proc_func = deprocess_img_vgg\n",
        "            self.pre_processing_func = vgg19.preprocess_input\n",
        "\n",
        "        elif model_name.lower() == 'vgg16':\n",
        "            base = vgg16.VGG16(input_shape = (self.h, self.w, 3), include_top = False, weights = 'imagenet')\n",
        "\n",
        "            self.content_layers = ['block5_conv3'] #, 'block5_conv2']\n",
        "            self.style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
        "\n",
        "            content_outputs = [base.get_layer(name).output for name in self.content_layers]\n",
        "            style_outputs = [base.get_layer(name).output for name in self.style_layers]\n",
        "            model_outputs = content_outputs + style_outputs\n",
        "\n",
        "            self.set_loss_weights(1e5, 1e-2, 1)\n",
        "            self.de_proc_func = deprocess_img_vgg\n",
        "            self.pre_processing_func = vgg16.preprocess_input\n",
        "\n",
        "        #create the model\n",
        "        self.model = Model(base.input, model_outputs)\n",
        "        self.model.summary()\n",
        "\n",
        "    def calc_loss(self, combined, gram_style_features, content_features):\n",
        "        '''\n",
        "        calculates the combined style and content loss\n",
        "\n",
        "        args -\n",
        "        combined : the output image\n",
        "        gram_style_features : gram matrices calculated for style feature maps\n",
        "        content_features : content feature maps\n",
        "        '''\n",
        "        #get model output for the combined image\n",
        "        model_op = self.model(combined)\n",
        "\n",
        "        #get the style and content output features for the combined image\n",
        "        style_op_features = model_op[len(self.content_layers):]\n",
        "        content_op_features = model_op[:len(self.content_layers)]\n",
        "\n",
        "        style_score = 0\n",
        "        content_score = 0\n",
        "\n",
        "        #calculate style score over the feature maps\n",
        "        weight_per_style_layer = 1. / float(len(self.style_layers))\n",
        "        for target_style, comb_style in zip(gram_style_features, style_op_features):\n",
        "            style_score = style_score + weight_per_style_layer * calc_style_loss(comb_style[0], target_style)\n",
        "\n",
        "        #calculate content score over the feature maps\n",
        "        weight_per_content_layer = 1. / float(len(self.content_layers))\n",
        "        for target_content, comb_content in zip(content_features, content_op_features):\n",
        "            content_score = content_score + weight_per_content_layer * calc_content_loss(comb_content[0], target_content)\n",
        "\n",
        "        style_loss = self.style_weight * style_score\n",
        "        content_loss = self.content_weight * content_score\n",
        "        loss = style_loss + content_loss\n",
        "\n",
        "        return loss, style_loss, content_loss\n",
        "\n",
        "    def create_ip(self):\n",
        "        '''\n",
        "        creates input to a model and performs preprocessing\n",
        "        steps for the content and style image\n",
        "        '''\n",
        "        #change to float and expand the dims\n",
        "        content_ip = pre_proc(self.content)\n",
        "        style_ip = pre_proc(self.style)\n",
        "\n",
        "        #preprocessing function of the trained model\n",
        "        content_ip = self.pre_processing_func(content_ip)\n",
        "        style_ip = self.pre_processing_func(style_ip)\n",
        "\n",
        "        return np.concatenate((style_ip, content_ip))\n",
        "\n",
        "    def combine(self, num_iter, lr, output_path, starting):\n",
        "        '''\n",
        "        function to combine the image. Calculates\n",
        "        and updates the image by calculating the gradients\n",
        "\n",
        "        args -\n",
        "        num_iter : number of iterations to run the update loop\n",
        "        lr : learing rate for the optimizer\n",
        "        output_path : path to save the final image\n",
        "        starting : to set the starting input as different images\n",
        "        '''\n",
        "        #expand dims and apply model pre processing\n",
        "\n",
        "        if starting == 'zeros':\n",
        "            combined = pre_proc(np.zeros_like(self.content))\n",
        "        elif starting == 'content':\n",
        "            combined = pre_proc(self.content)\n",
        "        elif starting == 'style':\n",
        "            combined = pre_proc(self.style)\n",
        "        elif starting == 'random':\n",
        "            combined = pre_proc(np.random.uniform(low = 0., high = 255.0, size = self.content.shape))\n",
        "        else :\n",
        "            raise Exception('{} no such starting option, please select from zeros, content, style or random'.format(starting))\n",
        "\n",
        "        combined = self.pre_processing_func(combined)\n",
        "        combined = tf.Variable(combined, dtype = tf.float32)\n",
        "\n",
        "        #get the style and content features\n",
        "        style_features, content_features = self.get_feature_maps()\n",
        "        #get the gram matrices of the style features using the style feature maps\n",
        "        gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]\n",
        "        #define out optimizer with given learning rate\n",
        "        opt = Adam(lr)\n",
        "\n",
        "        #values that the image needs to be clipped to after update\n",
        "        norm_means = np.array([103.939, 116.779, 123.68])\n",
        "        min_vals = -norm_means\n",
        "        max_vals = 255 - norm_means\n",
        "\n",
        "        img = None\n",
        "        intermediate_images = []\n",
        "        losses = []\n",
        "\n",
        "        for i in progressbar.progressbar(range(num_iter)):\n",
        "            #calculate loss and then the gradients for the combined image\n",
        "            with tf.GradientTape() as tape:\n",
        "                loss = self.calc_loss(combined, gram_style_features, content_features)\n",
        "                variation_loss = self.variation_weight * tf.image.total_variation(combined).numpy()[0]\n",
        "\n",
        "                total_loss, style_loss, content_loss = loss\n",
        "                grads = tape.gradient(total_loss + variation_loss, combined)\n",
        "\n",
        "            #apply the gradient updates\n",
        "            opt.apply_gradients([(grads, combined)])\n",
        "            clipped = tf.clip_by_value(combined, min_vals, max_vals)\n",
        "            combined.assign(clipped)\n",
        "\n",
        "            print (\" total loss:{} style_loss:{} content_loss:{} variation_loss:{}\".format(\n",
        "                                        total_loss, style_loss, content_loss, variation_loss))\n",
        "\n",
        "            losses.append(loss)\n",
        "            img = combined.numpy()\n",
        "            if i % (num_iter / 10) == 0:\n",
        "                intermediate_images.append(self.de_proc_func(img[0]))\n",
        "\n",
        "        plt.plot(range(0, num_iter), losses)\n",
        "        cv2.imwrite(output_path, self.de_proc_func(img[0]))\n",
        "        plt.savefig(\"loss.png\")\n",
        "\n",
        "        for i in range(0, len(intermediate_images)):\n",
        "            intermediate_images[i] = cv2.putText(intermediate_images[i], \"step {}\".format(i * int(num_iter / 10)),\n",
        "                                    (20, 70), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 0, 255), cv2.LINE_AA)\n",
        "\n",
        "        row_1 = intermediate_images[0]\n",
        "        for i in range(1, 5):\n",
        "            row_1 = np.hstack((row_1, intermediate_images[i]))\n",
        "\n",
        "        row_2 = intermediate_images[5]\n",
        "        for i in range(6, 10):\n",
        "            row_2 = np.hstack((row_2, intermediate_images[i]))\n",
        "\n",
        "        cv2.imwrite(\"intermediate_images.png\", np.vstack((row_1, row_2)))\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uk3l3AlHF4kw",
        "colab_type": "text"
      },
      "source": [
        "Now we can finally run style transfer on an image and check the results. Some examples are shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVc3DRG1z1p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_transfer_net = style_transfer('vgg19', 'content/content.jpg', 'styles/style.jpg')\n",
        "style_transfer_net.combine(500, 5, 'output.png', 'content')\n",
        "\n",
        "output = cv2.imread('output.png')\n",
        "intermediate_image = cv2.imread('intermediate_images.png')\n",
        "w = 512\n",
        "r = intermediate_image.shape[0] / w\n",
        "intermediate_image = cv2.resize(intermediate_image, (int(intermediate_image.shape[1] / r), w))\n",
        "cv2_imshow(output)\n",
        "\n",
        "print (\"\\n\\nthe intermediate outputs are :\")\n",
        "cv2_imshow(intermediate_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixXW5ekEGHzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "style_transfer_net = style_transfer('vgg19', 'content/content.jpg', 'styles/starry_night.jpg')\n",
        "style_transfer_net.combine(500, 5, 'output.png', 'content')\n",
        "\n",
        "output = cv2.imread('output.png')\n",
        "intermediate_image = cv2.imread('intermediate_images.png')\n",
        "w = 512\n",
        "r = intermediate_image.shape[0] / w\n",
        "intermediate_image = cv2.resize(intermediate_image, (int(intermediate_image.shape[1] / r), w))\n",
        "cv2_imshow(output)\n",
        "\n",
        "print (\"\\n\\nthe intermediate outputs are :\")\n",
        "cv2_imshow(intermediate_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vgk3cFCKThB",
        "colab_type": "text"
      },
      "source": [
        "You can try out some other samples as well if you want, just clear the notebook outputs once before running again. Just change the path of the images in the code above to run on a different sample. You can also download the code from the repo and run it on your system directly.\n",
        "\n",
        "https://github.com/Sambhav300899/Neural-Style-Transfer-TF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAfA10Uk9BX_",
        "colab_type": "text"
      },
      "source": [
        "**References** - \n",
        "* Deep Learning with Python by FranÃ§ois Chollet\n",
        "* https://arxiv.org/abs/1705.04058\n",
        "* https://towardsdatascience.comneural-style-transfer-tutorial-part-1-f5cd3315fa7f\n",
        "* https://www.tensorflow.org/tutorials/generative/style_transfer\n",
        "* https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916\n",
        "\n",
        "**Some issues with the current implementation** - \n",
        "* It can't be run on very high resolution content or the GPU runs out of memory.\n",
        "* This process can't be run in real time.\n",
        "* A lot of sharp gradients pop up around some objects.\n",
        "\n",
        "**Future Work** - \n",
        "* An image to image model can be trained on a set of styles and outputs generated with this technique to make it real time.\n",
        "* CycleGAN can be used for real time style transfer for a given style.\n",
        "* Figuring out a way to use with high resolution images(maybe downscale and then upscale with super resolution GAN, but that seems like a very naive solution).\n",
        "* Adding a quality measure of the style transfer.\n",
        "\n",
        "**Experiments which can be done** - \n",
        "* How changing the loss weights effects the output(a detailed study and not just changing and seeing visually).\n",
        "* Seeing how additional pre processing such as blurring before updating the image might help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8zArKI1GsWX",
        "colab_type": "text"
      },
      "source": [
        "**An experiment with starting points**       \n",
        "By default the content image is passed as the initial target image to calculate the loss. What if we used some different images as the starting points instead.\n",
        "\n",
        "**NOTE** - Please clear all outputs before running this or the notebook keeps getting disconnected. To clear all outputs (edit -> clear all outputs)\n",
        "\n",
        "**NOTE** - Also note that running the following cells will take some time as the number of iterations has to be increased"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbIfmz1eLFq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "lets try giving the starting image as style now\n",
        "'''\n",
        "style_transfer_net = style_transfer('vgg19', 'content/content.jpg', 'styles/style.jpg')\n",
        "style_transfer_net.combine(5000, 5, 'output.png', 'style')\n",
        "\n",
        "output = cv2.imread('output.png')\n",
        "intermediate_image = cv2.imread('intermediate_images.png')\n",
        "w = 512\n",
        "r = intermediate_image.shape[0] / w\n",
        "intermediate_image = cv2.resize(intermediate_image, (int(intermediate_image.shape[1] / r), w))\n",
        "cv2_imshow(output)\n",
        "\n",
        "print (\"\\n\\nthe intermediate outputs are :\")\n",
        "cv2_imshow(intermediate_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Qd7QQz0MExb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "lets try giving the starting image as zeros now\n",
        "'''\n",
        "style_transfer_net = style_transfer('vgg19', 'content/content.jpg', 'styles/style.jpg')\n",
        "style_transfer_net.combine(5000, 5, 'output.png', 'zeros')\n",
        "\n",
        "output = cv2.imread('output.png')\n",
        "intermediate_image = cv2.imread('intermediate_images.png')\n",
        "w = 512\n",
        "r = intermediate_image.shape[0] / w\n",
        "intermediate_image = cv2.resize(intermediate_image, (int(intermediate_image.shape[1] / r), w))\n",
        "cv2_imshow(output)\n",
        "\n",
        "print (\"\\n\\nthe intermediate outputs are :\")\n",
        "cv2_imshow(intermediate_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNUxmArqME_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "lets try giving the starting image as random now\n",
        "'''\n",
        "style_transfer_net = style_transfer('vgg19', 'content/content.jpg', 'styles/style.jpg')\n",
        "style_transfer_net.combine(5000, 5, 'output.png', 'random')\n",
        "\n",
        "output = cv2.imread('output.png')\n",
        "intermediate_image = cv2.imread('intermediate_images.png')\n",
        "w = 512\n",
        "r = intermediate_image.shape[0] / w\n",
        "intermediate_image = cv2.resize(intermediate_image, (int(intermediate_image.shape[1] / r), w))\n",
        "cv2_imshow(output)\n",
        "\n",
        "print (\"\\n\\nthe intermediate outputs are :\")\n",
        "cv2_imshow(intermediate_image)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}